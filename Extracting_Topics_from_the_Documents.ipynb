{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUq8yukdF8sKYbfxyGoZVd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prava0712/-Extracting-Topics-from-the-Documents/blob/main/Extracting_Topics_from_the_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task-1\n",
        "1. Load the dataset text_docs provided.\n",
        "2. Print the total number of rows and any relevant statistics, such as the number of unique\n",
        "documents.\n",
        "3. Identify any preprocessing steps that might be required"
      ],
      "metadata": {
        "id": "_uhmmB3IOEmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiUsBCx7Muoq",
        "outputId": "cfa59071-a7f1-45fb-d0e7-e23074e5daec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the dataset:\n",
            "   document_id                                               text\n",
            "0            1  The stock market has been experiencing volatil...\n",
            "1            2  The economy is growing, and businesses are opt...\n",
            "2            3  Climate change is a critical issue that needs ...\n",
            "3            4  Advances in artificial intelligence have revol...\n",
            "4            5  The rise of electric vehicles is shaping the f...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# 1. Load the dataset\n",
        "df = pd.read_csv('text_docs.csv')\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Print total number of rows and check for unique documents/missing values\n",
        "total_rows = len(df)\n",
        "unique_docs_count = df['text'].nunique()\n",
        "missing_values = df.isnull().sum()\n",
        "print(f\"\\nTotal number of rows (documents): {total_rows}\")\n",
        "print(f\"Number of unique documents: {unique_docs_count}\")\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(missing_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKgYLBa2NxW4",
        "outputId": "9c0182c4-06f1-40c2-88dd-074ef0aef431"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of rows (documents): 10\n",
            "Number of unique documents: 10\n",
            "\n",
            "Missing values per column:\n",
            "document_id    0\n",
            "text           0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Identify necessary preprocessing steps\n",
        "print(\"\\nRequired Preprocessing Steps for LDA:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REX6nNfAN4iE",
        "outputId": "73d83fd8-e9ee-4149-9a4c-d8cfd16c5be8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Required Preprocessing Steps for LDA:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Generate Topics Using LDA\n",
        "1. Prepare the data for LDA by creating a document-term matrix using a library like gensim\n",
        "or sklearn.\n",
        "2. Apply Latent Dirichlet Allocation (LDA) to extract topics from the dataset. Choose a\n",
        "suitable number of topics (e.g., 5).\n",
        "3. Display the top 5 words for each topic generated by the model"
      ],
      "metadata": {
        "id": "1qOfWtxHOHTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk\n",
        ">>> nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD2weiLvONkr",
        "outputId": "33fadf3a-fcd8-479e-a4fa-759b58e63739"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQmICJYgOZRg",
        "outputId": "f452a702-b3f6-4ddb-8f8d-e5285c093aa6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim import corpora\n",
        "# Ensure necessary NLTK downloads are completed (run once)\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('punkt_tab')\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess(text):\n",
        "  # 1. Lowercase\n",
        " text = text.lower()\n",
        " # 2. Remove punctuation and numbers\n",
        " text = re.sub(r'\\d+', '', text)\n",
        " text = text.translate(str.maketrans('', '', string.punctuation))\n",
        " # 3. Tokenize\n",
        " tokens = nltk.word_tokenize(text)\n",
        " # 4. Remove stopwords & 5. Lemmatization\n",
        " tokens = [lemmatizer.lemmatize(word) for word in tokens if word\n",
        "not in stop_words and len(word) > 2]\n",
        " return tokens\n",
        "# Apply preprocessing\n",
        "df['processed_text'] = df['text'].apply(preprocess)\n",
        "# Create a Dictionary and Corpus for Gensim\n",
        "dictionary = corpora.Dictionary(df['processed_text'])\n",
        "# Filter out extremes: words appearing in less than 2 documents or more than 50% of documents dictionary.filter_extremes(no_below=2, no_above=0.5) # Reduced no_below\n",
        "# Create the Corpus (Document-Term Matrix - BoW format)\n",
        "corpus = [dictionary.doc2bow(text) for text in df['processed_text']]\n",
        "print(f\"\\nTotal documents in Corpus: {len(corpus)}\")\n",
        "print(f\"Vocabulary size after filtering: {len(dictionary)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdh8uXs0Oedv",
        "outputId": "174ea232-c7ac-40df-9f80-82db24d0f558"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "\n",
            "Total documents in Corpus: 10\n",
            "Vocabulary size after filtering: 61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import LdaModel\n",
        "# Choose a suitable number of topics\n",
        "NUM_TOPICS = 5\n",
        "# Apply LDA\n",
        "lda_model = LdaModel(\n",
        " corpus=corpus,\n",
        " id2word=dictionary,\n",
        " num_topics=NUM_TOPICS,\n",
        " random_state=42,\n",
        " chunksize=100,\n",
        " passes=10,\n",
        " alpha='auto',\n",
        " per_word_topics=False\n",
        ")\n",
        "print(f\"\\nLDA Model trained successfully with {NUM_TOPICS} topics.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub16c1W_PKXp",
        "outputId": "9852763c-ee51-4f0a-d331-2e676e56fda0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LDA Model trained successfully with 5 topics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Top 5 Words for Each LDA Topic ---\")\n",
        "# Display the topics\n",
        "# 'num_words=5' specifies the top 5 words per topic\n",
        "for idx, topic in lda_model.print_topics(num_words=5):\n",
        " print(f\"Topic {idx}: {topic}\")\n",
        "print(\"\\n--- Explanation ---\")\n",
        "print(\"\"\"The words listed for each topic represent the most probable\n",
        "terms assigned to that theme. By examining these keywords, one can\n",
        "infer the subject matter of the topic (e.g., 'stock,' 'market,'\n",
        "'economy' suggests a 'Finance' topic').\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT1lCiSaPRIB",
        "outputId": "90cdc174-6a2a-4b8e-9672-d9a3ca8fc36f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Top 5 Words for Each LDA Topic ---\n",
            "Topic 0: 0.037*\"industry\" + 0.037*\"platform\" + 0.037*\"digital\" + 0.037*\"streaming\" + 0.037*\"world\"\n",
            "Topic 1: 0.016*\"industry\" + 0.016*\"future\" + 0.016*\"treatment\" + 0.016*\"platform\" + 0.016*\"optimistic\"\n",
            "Topic 2: 0.038*\"attention\" + 0.038*\"critical\" + 0.038*\"immediate\" + 0.038*\"climate\" + 0.038*\"global\"\n",
            "Topic 3: 0.062*\"cybersecurity\" + 0.062*\"become\" + 0.062*\"ongoing\" + 0.062*\"integrated\" + 0.062*\"concern\"\n",
            "Topic 4: 0.068*\"industry\" + 0.037*\"automobile\" + 0.037*\"uncertainty\" + 0.037*\"stock\" + 0.037*\"future\"\n",
            "\n",
            "--- Explanation ---\n",
            "The words listed for each topic represent the most probable\n",
            "terms assigned to that theme. By examining these keywords, one can\n",
            "infer the subject matter of the topic (e.g., 'stock,' 'market,'\n",
            "'economy' suggests a 'Finance' topic').\n"
          ]
        }
      ]
    }
  ]
}